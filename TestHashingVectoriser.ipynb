{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer, TfidfVectorizer\n",
    "from skl2onnx import convert_sklearn\n",
    "import skl2onnx\n",
    "from skl2onnx.common.data_types import StringTensorType\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = numpy.array([\n",
    "            \"This is the first document.\",\n",
    "            \"This document is the second document.\",\n",
    "            \"And this is the third one.\",\n",
    "            \"Is this the first document?\",\n",
    "        ]).reshape((4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = HashingVectorizer(ngram_range=(1, 1), norm=None, alternate_sign=False)\n",
    "cvect = CountVectorizer(ngram_range=(1, 1))\n",
    "tvect = TfidfVectorizer(ngram_range=(1, 1), use_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t2\n",
      "  (1, 5)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "Y = cvect.fit_transform(corpus.ravel())\n",
    "print(cvect.get_feature_names())\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 144749)\t1.0\n",
      "  (0, 170062)\t1.0\n",
      "  (0, 286878)\t1.0\n",
      "  (0, 351664)\t1.0\n",
      "  (0, 989160)\t1.0\n",
      "  (1, 144749)\t1.0\n",
      "  (1, 170062)\t1.0\n",
      "  (1, 286878)\t1.0\n",
      "  (1, 351664)\t2.0\n",
      "  (1, 544379)\t1.0\n",
      "  (2, 144749)\t1.0\n",
      "  (2, 170062)\t1.0\n",
      "  (2, 178949)\t1.0\n",
      "  (2, 180525)\t1.0\n",
      "  (2, 286878)\t1.0\n",
      "  (2, 948532)\t1.0\n",
      "  (3, 144749)\t1.0\n",
      "  (3, 170062)\t1.0\n",
      "  (3, 286878)\t1.0\n",
      "  (3, 351664)\t1.0\n",
      "  (3, 989160)\t1.0\n"
     ]
    }
   ],
   "source": [
    "X = vect.transform(corpus.ravel())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.4472135954999579\n",
      "  (0, 3)\t0.4472135954999579\n",
      "  (0, 6)\t0.4472135954999579\n",
      "  (0, 2)\t0.4472135954999579\n",
      "  (0, 1)\t0.4472135954999579\n",
      "  (1, 8)\t0.35355339059327373\n",
      "  (1, 3)\t0.35355339059327373\n",
      "  (1, 6)\t0.35355339059327373\n",
      "  (1, 1)\t0.7071067811865475\n",
      "  (1, 5)\t0.35355339059327373\n",
      "  (2, 8)\t0.4082482904638631\n",
      "  (2, 3)\t0.4082482904638631\n",
      "  (2, 6)\t0.4082482904638631\n",
      "  (2, 0)\t0.4082482904638631\n",
      "  (2, 7)\t0.4082482904638631\n",
      "  (2, 4)\t0.4082482904638631\n",
      "  (3, 8)\t0.4472135954999579\n",
      "  (3, 3)\t0.4472135954999579\n",
      "  (3, 6)\t0.4472135954999579\n",
      "  (3, 2)\t0.4472135954999579\n",
      "  (3, 1)\t0.4472135954999579\n"
     ]
    }
   ],
   "source": [
    "Z = tvect.fit_transform(corpus.ravel())\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/realno/sklearn-onnx/skl2onnx/operator_converters/text_vectoriser.py:171: UserWarning: Converter for TfidfVectorizer will use scikit-learn regular expression by default in version 1.6.\n",
      "  UserWarning)\n",
      "/home/realno/sklearn-onnx/skl2onnx/common/_container.py:519: UserWarning: Unable to find operator 'Tokenizer' in domain 'com.microsoft' in ONNX, op_version is forced to 1.\n",
      "  op_type, domain))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'\\x08\\x07\\x12\\x08skl2onnx\\x1a\\x051.6.0\"\\x07ai.onnx(\\x002\\x00:\\xa1\\x06\\n4\\n\\x05input\\n\\x0cshape_tensor\\x12\\tflattened\\x1a\\x07Reshape\"\\x07Reshape:\\x00\\nw\\n\\tflattened\\x12\\nnormalized\\x1a\\x10StringNormalizer\"\\x10StringNormalizer*\\x1e\\n\\x12case_change_action\"\\x05LOWER\\xa0\\x01\\x03*\\x18\\n\\x11is_case_sensitive\\x18\\x00\\xa0\\x01\\x02:\\x00\\n\\x8d\\x01\\n\\nnormalized\\x12\\ttokenized\\x1a\\tTokenizer\"\\tTokenizer*\\x0b\\n\\x04mark\\x18\\x00\\xa0\\x01\\x02*\\x11\\n\\nmincharnum\\x18\\x01\\xa0\\x01\\x02*\\x11\\n\\tpad_value\"\\x01#\\xa0\\x01\\x03*\\x1c\\n\\x08tokenexp\"\\r[a-zA-Z0-9_]+\\xa0\\x01\\x03:\\rcom.microsoft\\n+\\n\\ttokenized\\x12\\nflattened1\\x1a\\x07Flatten\"\\x07Flatten:\\x00\\n\\xd2\\x02\\n\\nflattened1\\x12\\x08variable\\x1a\\x0fTfIdfVectorizer\"\\x0fTfIdfVectorizer*\\x16\\n\\x0fmax_gram_length\\x18\\x01\\xa0\\x01\\x02*\\x15\\n\\x0emax_skip_count\\x18\\x00\\xa0\\x01\\x02*\\x16\\n\\x0fmin_gram_length\\x18\\x01\\xa0\\x01\\x02*\\r\\n\\x04mode\"\\x02TF\\xa0\\x01\\x03*\\x13\\n\\x0cngram_counts@\\x00\\xa0\\x01\\x07*$\\n\\rngram_indexes@\\x00@\\x01@\\x02@\\x03@\\x04@\\x05@\\x06@\\x07@\\x08\\xa0\\x01\\x07*J\\n\\x0cpool_stringsJ\\x03andJ\\x08documentJ\\x05firstJ\\x02isJ\\x03oneJ\\x06secondJ\\x03theJ\\x05thirdJ\\x04this\\xa0\\x01\\x08*9\\n\\x07weights=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?\\xa0\\x01\\x06:\\x00\\x12\\x0fCountVectorizer*\\x1e\\x08\\x01\\x10\\x07:\\n\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01B\\x0cshape_tensorZ\\x13\\n\\x05input\\x12\\n\\n\\x08\\x08\\x08\\x12\\x04\\n\\x02\\x08\\x01b\\x18\\n\\x08variable\\x12\\x0c\\n\\n\\x08\\x01\\x12\\x06\\n\\x00\\n\\x02\\x08\\tB\\x11\\n\\rcom.microsoft\\x10\\x01B\\x04\\n\\x00\\x10\\x0b'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_onnx_c = convert_sklearn(cvect, \"CountVectorizer\",\n",
    "                                     [(\"input\", StringTensorType([1]))])\n",
    "skl2onnx.helpers.onnx_helper.save_onnx_model(model_onnx_c, filename=\"count_vec.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/realno/sklearn-onnx/skl2onnx/operator_converters/hashing_vectoriser.py:102: UserWarning: Converter for HashingVectorizer will use scikit-learn regular expression by default in version 1.6.\n",
      "  UserWarning)\n",
      "/home/realno/sklearn-onnx/skl2onnx/common/_container.py:519: UserWarning: Unable to find operator 'HashingVectorizer' in domain 'com.microsoft' in ONNX, op_version is forced to 1.\n",
      "  op_type, domain))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'\\x08\\x06\\x12\\x08skl2onnx\\x1a\\x051.6.0\"\\x07ai.onnx(\\x002\\x00:\\xb1\\x04\\n4\\n\\x05input\\n\\x0cshape_tensor\\x12\\tflattened\\x1a\\x07Reshape\"\\x07Reshape:\\x00\\nw\\n\\tflattened\\x12\\nnormalized\\x1a\\x10StringNormalizer\"\\x10StringNormalizer*\\x1e\\n\\x12case_change_action\"\\x05LOWER\\xa0\\x01\\x03*\\x18\\n\\x11is_case_sensitive\\x18\\x00\\xa0\\x01\\x02:\\x00\\n\\x8d\\x01\\n\\nnormalized\\x12\\ttokenized\\x1a\\tTokenizer\"\\tTokenizer*\\x0b\\n\\x04mark\\x18\\x00\\xa0\\x01\\x02*\\x11\\n\\nmincharnum\\x18\\x01\\xa0\\x01\\x02*\\x11\\n\\tpad_value\"\\x01#\\xa0\\x01\\x03*\\x1c\\n\\x08tokenexp\"\\r[a-zA-Z0-9_]+\\xa0\\x01\\x03:\\rcom.microsoft\\n+\\n\\ttokenized\\x12\\nflattened1\\x1a\\x07Flatten\"\\x07Flatten:\\x00\\n_\\n\\nflattened1\\x12\\x08variable\\x1a\\x11HashingVectorizer\"\\x11HashingVectorizer*\\x12\\n\\nn_features\\x18\\x88\\'\\xa0\\x01\\x02:\\rcom.microsoft\\x12\\x11HashingVectorizer*\\x1e\\x08\\x01\\x10\\x07:\\n\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01B\\x0cshape_tensorZ\\x13\\n\\x05input\\x12\\n\\n\\x08\\x08\\x08\\x12\\x04\\n\\x02\\x08\\x01b\\x1a\\n\\x08variable\\x12\\x0e\\n\\x0c\\x08\\x01\\x12\\x08\\n\\x00\\n\\x04\\x08\\x81\\x80@B\\x04\\n\\x00\\x10\\x0bB\\x11\\n\\rcom.microsoft\\x10\\x01'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_onnx = convert_sklearn(vect, \"HashingVectorizer\",\n",
    "                                     [(\"input\", StringTensorType([1]))])\n",
    "model_onnx.ir_version=6\n",
    "skl2onnx.helpers.onnx_helper.save_onnx_model(model_onnx, \"hashing_vec.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x08\\x07\\x12\\x08skl2onnx\\x1a\\x051.6.0\"\\x07ai.onnx(\\x002\\x00:\\xae\\x07\\n.\\n\\x0ctfidf_output\\x12\\x08variable\\x1a\\x08Identity\"\\x08Identity:\\x00\\n4\\n\\x05input\\n\\x0cshape_tensor\\x12\\tflattened\\x1a\\x07Reshape\"\\x07Reshape:\\x00\\nw\\n\\tflattened\\x12\\nnormalized\\x1a\\x10StringNormalizer\"\\x10StringNormalizer*\\x1e\\n\\x12case_change_action\"\\x05LOWER\\xa0\\x01\\x03*\\x18\\n\\x11is_case_sensitive\\x18\\x00\\xa0\\x01\\x02:\\x00\\n\\x8d\\x01\\n\\nnormalized\\x12\\ttokenized\\x1a\\tTokenizer\"\\tTokenizer*\\x0b\\n\\x04mark\\x18\\x00\\xa0\\x01\\x02*\\x11\\n\\nmincharnum\\x18\\x01\\xa0\\x01\\x02*\\x11\\n\\tpad_value\"\\x01#\\xa0\\x01\\x03*\\x1c\\n\\x08tokenexp\"\\r[a-zA-Z0-9_]+\\xa0\\x01\\x03:\\rcom.microsoft\\n+\\n\\ttokenized\\x12\\nflattened1\\x1a\\x07Flatten\"\\x07Flatten:\\x00\\n\\xda\\x02\\n\\nflattened1\\x12\\x10count_vec_output\\x1a\\x0fTfIdfVectorizer\"\\x0fTfIdfVectorizer*\\x16\\n\\x0fmax_gram_length\\x18\\x01\\xa0\\x01\\x02*\\x15\\n\\x0emax_skip_count\\x18\\x00\\xa0\\x01\\x02*\\x16\\n\\x0fmin_gram_length\\x18\\x01\\xa0\\x01\\x02*\\r\\n\\x04mode\"\\x02TF\\xa0\\x01\\x03*\\x13\\n\\x0cngram_counts@\\x00\\xa0\\x01\\x07*$\\n\\rngram_indexes@\\x00@\\x01@\\x02@\\x03@\\x04@\\x05@\\x06@\\x07@\\x08\\xa0\\x01\\x07*J\\n\\x0cpool_stringsJ\\x03andJ\\x08documentJ\\x05firstJ\\x02isJ\\x03oneJ\\x06secondJ\\x03theJ\\x05thirdJ\\x04this\\xa0\\x01\\x08*9\\n\\x07weights=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?=\\x00\\x00\\x80?\\xa0\\x01\\x06:\\x00\\nS\\n\\x10count_vec_output\\x12\\x0ctfidf_output\\x1a\\nNormalizer\"\\nNormalizer*\\r\\n\\x04norm\"\\x02L2\\xa0\\x01\\x03:\\nai.onnx.ml\\x12\\x0fTfidfVectorizer*\\x1e\\x08\\x01\\x10\\x07:\\n\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x01B\\x0cshape_tensorZ\\x13\\n\\x05input\\x12\\n\\n\\x08\\x08\\x08\\x12\\x04\\n\\x02\\x08\\x01b\\x18\\n\\x08variable\\x12\\x0c\\n\\n\\x08\\x01\\x12\\x06\\n\\x00\\n\\x02\\x08\\tB\\x04\\n\\x00\\x10\\x0bB\\x11\\n\\rcom.microsoft\\x10\\x01B\\x0e\\n\\nai.onnx.ml\\x10\\x01'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_onnx_t = convert_sklearn(tvect, \"TfidfVectorizer\",\n",
    "                                     [(\"input\", StringTensorType([1]))])\n",
    "skl2onnx.helpers.onnx_helper.save_onnx_model(model_onnx_t, filename=\"tfidf_vec.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir_version: 7\n",
      "producer_name: \"skl2onnx\"\n",
      "producer_version: \"1.6.0\"\n",
      "domain: \"ai.onnx\"\n",
      "model_version: 0\n",
      "doc_string: \"\"\n",
      "graph {\n",
      "  node {\n",
      "    input: \"input\"\n",
      "    input: \"shape_tensor\"\n",
      "    output: \"flattened\"\n",
      "    name: \"Reshape\"\n",
      "    op_type: \"Reshape\"\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"flattened\"\n",
      "    output: \"normalized\"\n",
      "    name: \"StringNormalizer\"\n",
      "    op_type: \"StringNormalizer\"\n",
      "    attribute {\n",
      "      name: \"case_change_action\"\n",
      "      s: \"LOWER\"\n",
      "      type: STRING\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"is_case_sensitive\"\n",
      "      i: 0\n",
      "      type: INT\n",
      "    }\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"normalized\"\n",
      "    output: \"tokenized\"\n",
      "    name: \"Tokenizer\"\n",
      "    op_type: \"Tokenizer\"\n",
      "    attribute {\n",
      "      name: \"mark\"\n",
      "      i: 0\n",
      "      type: INT\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"mincharnum\"\n",
      "      i: 1\n",
      "      type: INT\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"pad_value\"\n",
      "      s: \"#\"\n",
      "      type: STRING\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"tokenexp\"\n",
      "      s: \"[a-zA-Z0-9_]+\"\n",
      "      type: STRING\n",
      "    }\n",
      "    domain: \"com.microsoft\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"tokenized\"\n",
      "    output: \"flattened1\"\n",
      "    name: \"Flatten\"\n",
      "    op_type: \"Flatten\"\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"flattened1\"\n",
      "    output: \"variable\"\n",
      "    name: \"TfIdfVectorizer\"\n",
      "    op_type: \"TfIdfVectorizer\"\n",
      "    attribute {\n",
      "      name: \"max_gram_length\"\n",
      "      i: 1\n",
      "      type: INT\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"max_skip_count\"\n",
      "      i: 0\n",
      "      type: INT\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"min_gram_length\"\n",
      "      i: 1\n",
      "      type: INT\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"mode\"\n",
      "      s: \"TF\"\n",
      "      type: STRING\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"ngram_counts\"\n",
      "      ints: 0\n",
      "      type: INTS\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"ngram_indexes\"\n",
      "      ints: 0\n",
      "      ints: 1\n",
      "      ints: 2\n",
      "      ints: 3\n",
      "      ints: 4\n",
      "      ints: 5\n",
      "      ints: 6\n",
      "      ints: 7\n",
      "      ints: 8\n",
      "      type: INTS\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"pool_strings\"\n",
      "      strings: \"and\"\n",
      "      strings: \"document\"\n",
      "      strings: \"first\"\n",
      "      strings: \"is\"\n",
      "      strings: \"one\"\n",
      "      strings: \"second\"\n",
      "      strings: \"the\"\n",
      "      strings: \"third\"\n",
      "      strings: \"this\"\n",
      "      type: STRINGS\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"weights\"\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      type: FLOATS\n",
      "    }\n",
      "    domain: \"\"\n",
      "  }\n",
      "  name: \"CountVectorizer\"\n",
      "  initializer {\n",
      "    dims: 1\n",
      "    data_type: 7\n",
      "    int64_data: -1\n",
      "    name: \"shape_tensor\"\n",
      "  }\n",
      "  input {\n",
      "    name: \"input\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 8\n",
      "        shape {\n",
      "          dim {\n",
      "            dim_value: 1\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  output {\n",
      "    name: \"variable\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 1\n",
      "        shape {\n",
      "          dim {\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 9\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "opset_import {\n",
      "  domain: \"com.microsoft\"\n",
      "  version: 1\n",
      "}\n",
      "opset_import {\n",
      "  domain: \"\"\n",
      "  version: 11\n",
      "}\n",
      "\n",
      "ir_version: 7\n",
      "producer_name: \"skl2onnx\"\n",
      "producer_version: \"1.6.0\"\n",
      "domain: \"ai.onnx\"\n",
      "model_version: 0\n",
      "doc_string: \"\"\n",
      "graph {\n",
      "  node {\n",
      "    input: \"tfidf_output\"\n",
      "    output: \"variable\"\n",
      "    name: \"Identity\"\n",
      "    op_type: \"Identity\"\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"input\"\n",
      "    input: \"shape_tensor\"\n",
      "    output: \"flattened\"\n",
      "    name: \"Reshape\"\n",
      "    op_type: \"Reshape\"\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"flattened\"\n",
      "    output: \"normalized\"\n",
      "    name: \"StringNormalizer\"\n",
      "    op_type: \"StringNormalizer\"\n",
      "    attribute {\n",
      "      name: \"case_change_action\"\n",
      "      s: \"LOWER\"\n",
      "      type: STRING\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"is_case_sensitive\"\n",
      "      i: 0\n",
      "      type: INT\n",
      "    }\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"normalized\"\n",
      "    output: \"tokenized\"\n",
      "    name: \"Tokenizer\"\n",
      "    op_type: \"Tokenizer\"\n",
      "    attribute {\n",
      "      name: \"mark\"\n",
      "      i: 0\n",
      "      type: INT\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"mincharnum\"\n",
      "      i: 1\n",
      "      type: INT\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"pad_value\"\n",
      "      s: \"#\"\n",
      "      type: STRING\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"tokenexp\"\n",
      "      s: \"[a-zA-Z0-9_]+\"\n",
      "      type: STRING\n",
      "    }\n",
      "    domain: \"com.microsoft\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"tokenized\"\n",
      "    output: \"flattened1\"\n",
      "    name: \"Flatten\"\n",
      "    op_type: \"Flatten\"\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"flattened1\"\n",
      "    output: \"count_vec_output\"\n",
      "    name: \"TfIdfVectorizer\"\n",
      "    op_type: \"TfIdfVectorizer\"\n",
      "    attribute {\n",
      "      name: \"max_gram_length\"\n",
      "      i: 1\n",
      "      type: INT\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"max_skip_count\"\n",
      "      i: 0\n",
      "      type: INT\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"min_gram_length\"\n",
      "      i: 1\n",
      "      type: INT\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"mode\"\n",
      "      s: \"TF\"\n",
      "      type: STRING\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"ngram_counts\"\n",
      "      ints: 0\n",
      "      type: INTS\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"ngram_indexes\"\n",
      "      ints: 0\n",
      "      ints: 1\n",
      "      ints: 2\n",
      "      ints: 3\n",
      "      ints: 4\n",
      "      ints: 5\n",
      "      ints: 6\n",
      "      ints: 7\n",
      "      ints: 8\n",
      "      type: INTS\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"pool_strings\"\n",
      "      strings: \"and\"\n",
      "      strings: \"document\"\n",
      "      strings: \"first\"\n",
      "      strings: \"is\"\n",
      "      strings: \"one\"\n",
      "      strings: \"second\"\n",
      "      strings: \"the\"\n",
      "      strings: \"third\"\n",
      "      strings: \"this\"\n",
      "      type: STRINGS\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"weights\"\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      floats: 1.0\n",
      "      type: FLOATS\n",
      "    }\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"count_vec_output\"\n",
      "    output: \"tfidf_output\"\n",
      "    name: \"Normalizer\"\n",
      "    op_type: \"Normalizer\"\n",
      "    attribute {\n",
      "      name: \"norm\"\n",
      "      s: \"L2\"\n",
      "      type: STRING\n",
      "    }\n",
      "    domain: \"ai.onnx.ml\"\n",
      "  }\n",
      "  name: \"TfidfVectorizer\"\n",
      "  initializer {\n",
      "    dims: 1\n",
      "    data_type: 7\n",
      "    int64_data: -1\n",
      "    name: \"shape_tensor\"\n",
      "  }\n",
      "  input {\n",
      "    name: \"input\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 8\n",
      "        shape {\n",
      "          dim {\n",
      "            dim_value: 1\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  output {\n",
      "    name: \"variable\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 1\n",
      "        shape {\n",
      "          dim {\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 9\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "opset_import {\n",
      "  domain: \"\"\n",
      "  version: 11\n",
      "}\n",
      "opset_import {\n",
      "  domain: \"com.microsoft\"\n",
      "  version: 1\n",
      "}\n",
      "opset_import {\n",
      "  domain: \"ai.onnx.ml\"\n",
      "  version: 1\n",
      "}\n",
      "\n",
      "ir_version: 6\n",
      "producer_name: \"skl2onnx\"\n",
      "producer_version: \"1.6.0\"\n",
      "domain: \"ai.onnx\"\n",
      "model_version: 0\n",
      "doc_string: \"\"\n",
      "graph {\n",
      "  node {\n",
      "    input: \"input\"\n",
      "    input: \"shape_tensor\"\n",
      "    output: \"flattened\"\n",
      "    name: \"Reshape\"\n",
      "    op_type: \"Reshape\"\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"flattened\"\n",
      "    output: \"normalized\"\n",
      "    name: \"StringNormalizer\"\n",
      "    op_type: \"StringNormalizer\"\n",
      "    attribute {\n",
      "      name: \"case_change_action\"\n",
      "      s: \"LOWER\"\n",
      "      type: STRING\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"is_case_sensitive\"\n",
      "      i: 0\n",
      "      type: INT\n",
      "    }\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"normalized\"\n",
      "    output: \"tokenized\"\n",
      "    name: \"Tokenizer\"\n",
      "    op_type: \"Tokenizer\"\n",
      "    attribute {\n",
      "      name: \"mark\"\n",
      "      i: 0\n",
      "      type: INT\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"mincharnum\"\n",
      "      i: 1\n",
      "      type: INT\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"pad_value\"\n",
      "      s: \"#\"\n",
      "      type: STRING\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"tokenexp\"\n",
      "      s: \"[a-zA-Z0-9_]+\"\n",
      "      type: STRING\n",
      "    }\n",
      "    domain: \"com.microsoft\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"tokenized\"\n",
      "    output: \"flattened1\"\n",
      "    name: \"Flatten\"\n",
      "    op_type: \"Flatten\"\n",
      "    domain: \"\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"flattened1\"\n",
      "    output: \"variable\"\n",
      "    name: \"HashingVectorizer\"\n",
      "    op_type: \"HashingVectorizer\"\n",
      "    attribute {\n",
      "      name: \"n_features\"\n",
      "      i: 5000\n",
      "      type: INT\n",
      "    }\n",
      "    domain: \"com.microsoft\"\n",
      "  }\n",
      "  name: \"HashingVectorizer\"\n",
      "  initializer {\n",
      "    dims: 1\n",
      "    data_type: 7\n",
      "    int64_data: -1\n",
      "    name: \"shape_tensor\"\n",
      "  }\n",
      "  input {\n",
      "    name: \"input\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 8\n",
      "        shape {\n",
      "          dim {\n",
      "            dim_value: 1\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  output {\n",
      "    name: \"variable\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 1\n",
      "        shape {\n",
      "          dim {\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 1048577\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "opset_import {\n",
      "  domain: \"\"\n",
      "  version: 11\n",
      "}\n",
      "opset_import {\n",
      "  domain: \"com.microsoft\"\n",
      "  version: 1\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_onnx_c)\n",
    "print(model_onnx_t)\n",
    "print(model_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load('squeezenet.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(onnx_model.ir_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
